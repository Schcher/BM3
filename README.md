1、LightGCN的简化设计： LightGCN通过去除GCN的特征变换和非线性激活层来简化结构，这使得它在推荐任务中更加高效，因为这些层可能会对性能产生负面影响。  
2、读出和残差连接：LightGCN通过聚合不同层次的嵌入来获取最终的用户和物品表示，并通过在物品嵌入中加入残差连接解决GCN的过度平滑问题，从而保留更多的初始信息。  
3、多模态对比损失：BM3采用对比学习策略，通过生成对比视图和应用对比损失，避免模型陷入平凡解。同时，为了减少计算复杂度，BM3使用简单的嵌入丢弃技术而非复杂的图增强。  
4、梯度停止操作（Stop-gradient）：为了避免模型陷入常量解的问题，BM3采用了停止梯度策略，确保某些网络部分在训练过程中不会接收梯度，从而保持稳定的训练过程。
5、推荐系统中的Top-K推荐：BM3最终会根据用户和物品的嵌入进行内积操作，计算出用户对每个候选物品的评分，并选取前K个得分最高的物品作为推荐结果。


项目介绍：传统的推荐系统主要依赖于用户的历史交互数据，而忽略了物品的丰富多模态内容（图片、文本）。BM3模型通过整合这些多模态信息，提供更准确、更丰富的推荐。

职责描述：

LightGCN的简化设计： 通过去除GCN的特征变换和非线性激活层来简化结构；

Readout函数和残差连接：通过聚合不同层次的嵌入来获取最终的用户和物品表示，并在物品嵌入中加入残差连接解决LinghtGCN的过度平滑问题；

多模态对比损失：BM3采用对比学习策略，通过生成对比视图和应用对比损失，避免模型陷入平凡解；

梯度停止操作（Stop-gradient）：为了避免模型陷入常量解的问题，BM3采用了停止梯度策略，

Top-K推荐：BM3最终会根据用户和物品的嵌入进行内积操作，计算出用户对每个候选物品的评分，并选取前K个得分最高的物品作为推荐结果。


 为了生成用户的物品推荐，首先我们预测用户和候选物品之间的交互得分。然后，根据预测的交互得分对候选物品进行降序排列，并选择前K个排名最高的物品作为对用户的推荐  
 由于我们的MMCL（多模态对比学习方法）可以在用户和物品的潜在嵌入上学习到一个好的预测器，因此我们使用预测器𝑓𝑝转化后的嵌入进行内积计算。  

数据集：
● 数据集选择：作者根据之前的研究，选择了 Amazon 评论数据集中的三个类别——Baby、Sports and Outdoors（简称为 Sports）和 Electronics 进行实验。这些数据集提供了同时包含产品描述和图像信息的丰富数据。
● 实验评估目标：选择这些数据集的目的是为了确保可以在大规模数据集上对比尽可能多的基准模型，从而评估 BM3 模型的性能。
● 数据处理方式：
● 正交互记录：每个评论的评分都被视为用户与物品之间的正交互记录，这种方法在推荐系统的研究中非常常见。
● 5-core 设置：为了确保每个用户和物品都有足够的交互记录，数据集经过了 5-core 设置的预处理，即仅保留至少有 5 条交互记录的用户和物品。
● 数据稀疏度：稀疏度通过交互次数除以用户数量与物品数量的乘积来衡量，表明数据集中用户与物品的交互程度。
● 多模态特征：
● 视觉模态：使用了之前研究中提取并发布的 4,096 维视觉特征。这些特征可能来自产品的图像数据。
● 文本模态：通过拼接物品的标题、描述、类别和品牌，使用句子转换器生成 384 维的句子嵌入，这些嵌入表示产品的文本描述。


利用 物品的多模态信息来提高推荐的准确性。
现有的最先进方法除了使用用户-物品交互图外，还使用 辅助图（用户-用户 或 物品-物品 关系图）来增强用户或物品的表示。
这些表示通常在辅助图上通过图卷积进行传播和聚合，但在大规模图上的计算和内存开销会非常大。
现有的多模态推荐方法通常在贝叶斯个性化排序（BPR）损失中使用随机采样的负样本来指导用户或物品表示的学习，这不仅增加了在大图上的计算成本，还可能在训练过程中引入噪声监督信号。（ 具体来说，BPR 损失的核心思想是：对于每个用户，模型应该给用户实际喜欢的物品（正例）分配比用户不喜欢的物品（负例）更高的分数。为了实现这一点，BPR 通过从用户没有互动过的物品集合中随机选择一些负例，来和用户实际喜欢的物品（正例）进行对比。通过这种方式，模型会学到用户对哪些物品有偏好，并通过优化损失函数来提高推荐精度。   在 BPR 损失中，通过从用户未接触过的物品中随机选择负例，并将这些负例与用户实际喜欢的物品进行比较，从而引导模型学习用户和物品的表示（即模型如何表示用户和物品的特征）。这种方式帮助模型区分用户喜欢和不喜欢的物品，以此提高推荐的准确性。  ）
利用历史的 用户-物品交互数据来建模用户对物品得到偏好，并向用户推荐物品。然而物品中的丰富的多模态内容信息（文本、图像、视频）尚未得到充分的探索。
多模态推荐研究  探讨了 将物品的多模态信息整合到传统的 用户-物品推荐框架中。
将多模态特征与物品的潜在表示连接在一起  或 使用注意力机制来捕捉用户对物品多模态特征的偏好。
基于GNN多模态方法的缺陷：
基于GNN可以实现最先进的推荐准确性，但在涉及大规模的场景中，会出现一些问题。首先，这些方法通常基于成对排序损失（例如BPR）来学习用户和物品的表示，该损失将观察到的 用户-物品交互对 是为正样本 ， 而将随机采样的用户-物品对 视为负样本。这种负样本采样策略在大规模图上可能导致难以承受的成本，并将噪声监督信号带入训练过程中。
 例如 LightGCN [10] 中，默认的均匀采样 [42] 占据了每个训练周期超过 25% 的时间。其次，利用辅助图结构的方法在构建和/或在大规模辅助图上训练时，可能会产生难以承受的内存成本。  

自监督：
提供一种无需负样本来学习用户和物品表示的解决方案。ssl可以实现与监督学习相媲美甚至更好的结果。ssl主要思想是通过 两个不对称的网络（在线网络 和 目标网络）来最大化从不同的失真版本的样本中获得表示的的相似性。然而，仅使用正样本进行训练会导致模型陷入平凡的常数解。为了解决这个坍塌问题，BYOL和simsiam为在线网络引入了一个额外的 预测器 网络 ， 在目标网络上使用了特殊的  停止梯度 的操作。最近  BUIR 将 BYOL 转移到推荐领域上。

BM3
提出了一种 自引导多模态模型（BM3),用于多模态推荐。BM3 移除了 SSL 框架中的 目标网络 和引入dropout机制，简化了 模型的结构，从而减少了一半的模型参数。 也不需要传统的图增强技术，从而进一步降低了内存和计算开销。最后，BM3 设计了一个专门的损失函数，既能重构用户-物品交互图，又能在跨模态和模态内对齐学习到的特征。

其他相关的推荐系统算法：
大多数早期的多模态推荐模型 在利用深度学习 在 CF 框架的基础上探索用户的偏好。
VBPR：在BPR方法的基础上，VBPR利用CNN提取物品的视觉特征。并将这些特征与物品ID嵌入拼接，VBPR实现了物品的表示和预测。
Deepstyle：在BPR框架下，Deepstyle  通过同时使用视觉和风格特征来增强物品的表示。
注意力机制：
● VECF：利用 VGG 模型对图像进行分割，捕捉用户对图像不同区域的注意力。
● MAML：使用两层神经网络捕捉用户对物品的文本和视觉特征的偏好。

基于图的多模态模型：
通过将 用户-物品交互图和辅助图中的结构信息纳入模型。

为了利用物品的多模态信息，MMGCN [34] 采用了图卷积网络（GCNs）的信息传递机制，并构建了一个特定模态的用户-物品二分图，这样可以从多跳邻居处捕获信息，以增强用户和物品的表示。在 MMGCN 的基础上，DualGNN [31] 引入了一个用户共现图和一个模型偏好学习模块，以捕捉用户对物品不同模态特征的偏好。由于用户-物品图可能包含意外的交互，GRCN [33] 引入了一个图优化层，通过识别噪声边和修正假阳性边来优化用户-物品交互图的结构。为了显式地挖掘物品之间的语义信息，LATTICE [39] 为每种模态构建物品-物品关系图，并将这些图融合在一起以获得潜在的物品图。它在使用 GCNs 传播和聚合物品信息后，动态更新图。FREEDOM [45] 进一步检测到物品-物品图的学习是微不足道的，并冻结图以实现有效和高效的推荐。文献 [43] 提供了关于多模态推荐系统的综合调查，包括分类、评估和未来方向。
尽管基于图的多模态模型在推荐准确性方面达到了新的最先进水平，但它们通常需要辅助图来进行用户和物品的增强，还需要大量的负样本来通过 BPR 损失进行表示学习。这两种要求都可能导致高计算复杂性和难以承受的内存成本，尤其是在涉及大规模图的场景中，这限制了这些模型的效率。

自监督学习（SSL):
当前的 SSL框架源自于 孪生网络 。
BYOL 使用两个耦合的编码器（在线编码器和目标编码器），通过迭代优化和更新来避免网络崩溃。目标编码器是在线编码器的指数移动平均。
simsiam：验证了 ‘停止梯度’操作对于防止网络崩溃的关键作用，同时在线和目标编码器共享参数。
Barlow Twins：采用对称架构，通过创新的目标函数使交叉相关矩阵尽可能接近单位矩阵。  

流程：
由于多模态的特征空间彼此不同，首先要将多模态特征和ID嵌入转换为相同的潜在空间。对于一个多模态特征向量 em 使用MLP 投影到低维空间。
ID嵌入：尽管用户和物品的 ID嵌入可以直接初始化在潜在空间，但是它不包含用户-物品交互图的结构信息。使用lightGCN作为骨干网络，并采用残差连接来编码用户-物品交互图的结构。（ 多模态特征通过MLP转换到相同的低维潜在空间，然后ID嵌入通过图卷积网络（GCN）进行更新，以捕获用户-物品交互的结构信息。这种方法不仅能够处理不同模态特征之间的差异，还能充分利用用户与物品之间的复杂关系进行推荐。  


为什么 设计 LightGCN
在基础的GCN之上，lightGCN通过移除特征变换Wl和非线性激活层 简化其结构 ， 因为这些层对推荐性能有负面影响（在推荐系统场景中，用户与物品的交互通常已经能直接反映重要的关系。过多的特征变换可能会掩盖或扭曲这种直接的关系，导致性能下降。非线性激活通常是引入非线性能力，使得模型能够处理更复杂的模式，但是对于推荐系统来说，用户和物品之间的交互往往是比较明确和线性的，过多的非线性会导致信息的扭曲，尤其是在多层的网络中，可能会使得有用的信号被过度平滑或弱化。


损失函数
图重构损失：通过最大化用户和物品之间的正向扰动相似度来优化模型
跨模态特征对齐损失：通过最小化物品ID嵌入和其多模态特征之间的差异，来对齐不同模态下的特征表示。鼓励具有相似多模态特征的物品ID嵌入彼此靠近。
模态内特征掩码损失：通过随机掩码一部分特征来强化模型对稀疏表示的学习

● 函数 C(hu,hi)C(h_u, h_i)C(hu,hi)：这是一个负余弦相似度度量，用来表示用户 uuu 和物品 iii 之间的相似性。其目标是最大化正样本的相似性，即希望给定用户 uuu 时，正确预测正样本物品 iii 的可能性越高越好。此相似度的最小值为 -1，这意味着两个嵌入完全相反。
● 停止梯度操作：为了避免训练中的目标网络接收梯度，作者在目标网络上使用了停止梯度操作，确保梯度仅在在线网络上传播。（在BM3模型中，使用停止梯度是为了确保模型在自监督学习过程中能够学习到高质量的特征表示，同时保持目标网络的稳定性，避免由于梯度直接回传导致的模型性能问题。通过这种方式，BM3能够有效地利用正样本信息，而无需依赖负样本进行训练。）
● 跨模态特征对齐损失：此部分损失用于将多模态特征与目标 ID 嵌入对齐，使得具有相似多模态特征的物品在嵌入空间中更接近。
● 模态内特征掩码损失：通过随机掩码一些嵌入特征，这种损失鼓励模型学习更稀疏的表示，这在处理大规模数据时更为有效。
● 最终损失函数：所有上述损失函数的组合，加上在线嵌入的正则化惩罚，构成了模型的最终优化目标。
